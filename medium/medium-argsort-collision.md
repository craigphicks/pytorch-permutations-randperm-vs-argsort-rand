# Permutation Generation in PyTorch on GPU: Statistic Based Decision Rule for `randperm` vs. `argsort` and `rand`

## 1. Introduction

The PyTorch library, widely used for neural networks on a GPU, supplies a function `torch.randperm(length)` that returns a permutation in a one-dimensional tensor, e.g.,
```python
>>> torch.randperm(6)
tensor([5, 3, 2, 4, 0, 1])
```
However, `torch.randperm` has a drawback - it cannot produce multiple permutations in batch mode. In PyTorch, batch mode refers to processing multiple inputs simultaneously, which improves performance.

A similar result can be obtained by using `torch.argsort` with `torch.rand` or `torch.randint`:
```python
>>> torch.argsort(torch.rand(6))
tensor([4, 3, 2, 5, 0, 1])
```
Unlike `torch.randperm`, `torch.argsort` with `torch.rand` or `torch.randint` can be used to produce a multidimensional tensor of permutations:
```python
# Create a batch of 3 permutations of length 6
>>> torch.argsort(torch.rand(3, 6))
tensor([[4, 3, 2, 5, 0, 1],
        [2, 5, 1, 0, 3, 4],
        [0, 3, 1, 5, 2, 4]])
```

The difference in speed becomes significant when utilizing a GPU to process tensors in parallel in a batch call, which may be faster per permutation by orders of magnitude. However, if there is no need to create multiple permutations or no need for speed, then using `randperm` is sufficient.

## 2. Bias in Permutations Generated by `torch.argsort` with `torch.rand`

How does `torch.randperm` differ from `torch.argsort(torch.rand())` and `torch.argsort(torch.randint())` and why is it considered better?

The issue is that among the arrays of random numbers produced by `torch.rand` and `torch.randint` are arrays that include duplicate numbers (also known as collisions). That, in turn, biases the distribution of permutations produced by sorting those arrays with `torch.sort`.

```python
>>> a = torch.randint(low=10, high=16, size=(6,))
tensor([11, 12, 15, 12, 11, 10])
>>> torch.sort(a, stable=True)
torch.return_types.sort(
values=tensor([10, 11, 11, 12, 12, 15]),
indices=tensor([5, 0, 4, 1, 3, 2]))
```

In the above example, the value of the parameter range `high - low` was set artificially low to 6 to induce duplicate values to be sorted. As a result, the sorted indices `0, 4`, corresponding to the duplicate sorted values `11, 11`, are arranged in ascending order (guaranteed by a stable sort, which maintains the relative order of equal elements). Likewise, indices `1, 3` are in ascending order. The resulting distribution of sorted indices is not symmetric because it is biased to have more ascending pairs than descending pairs, as a result of duplicates.

In contrast, the function `torch.randperm` is written to detect any duplicate keys and partially recalculate the results to remove the effects of those duplicate keys. The recalculation consists of regenerating keys for, and resorting only, those intervals with compromised values after the initial sort is complete. The result is almost perfect (excepting the tiny chance of the regenerated keys also having duplicates - it doesn't use a loop to eliminate that possibility). By using `torch.randperm`, the possibility of a biased distribution is (almost) eliminated.

To summarize:

- `randperm` ensures unbiased permutations but can't be batched efficiently on GPU
- `argsort(rand())` can be batched but has bias due to collisions

## 3. A criterion for evaluating the bias of `argsort` with `rand`/`randint`

Let **K** be the length of the permutations we want to generate.

Let **n** in **{24, 32, 64}** be the number of bits used in the random number generation.  `rand` uses 24, and `randint` can use 32 or 64.  (Intermediate values for ##m## are possible but suboptimal.)

Define a statistical measure **P_crit(K)** such that given a random permutation generated by either `randperm(K)` or `torch.argsort(torch.rand(K))` with a 50% chance of each method being used, but without knowing which method was used. **P_crit(K)** is the probability of guessing correctly which distribution the sample was drawn from, using an optimal guessing strategy.

We then constrain the value of **P_crit** to define our desired risk model. For example, setting **P_crit = 0.55** means we would be able to correctly guess whether a permutation came from the fair or biased distribution 11 out of 20 times, or 55% percent of the time. The possible range of **P_crit** is between **0.5** to **1**, corresponding to 50% to 100%. However, we will only consider the range **0.50** to **0.55**, because beyond that the model loses accuracy.

It can be shown that **P_crit** is adequately approximated by 

$$
P_{\text{crit}}(N,K,m) = \text{cdf}(\frac{\sqrt{NK}\ (K-1)}{2^{m+3}}; 0, 1)
\tag{1}
$$

where 

- **m** is the number of bits of the type used for random number generation, i.e., **24**, **32**, or **64**, corresponding to `rand(K)`, `randint(K,...,dtype=torch.integer32)`, or `randint(K,...,dtype=torch.integer64)`.

- **N** is a free parameter representing the number of permutations in the statistical ensemble to be evaluated. **N=1** is a typical value, even when more than one permutation is being generated.  (The number generated does not need to be equal to number being evaluated).

- **cdf(...;0,1)** is the cumulative distribution function of a unit guassian distribution.

Approximating $K-1$ as $K$, **log2(K)** is given by

<!-- 
eq2 = c - sqrt(2) * erfinv(2*p - 1)
eq8 = lnk + lnn/3 - 2/3 * (log(c) + log(s)) 
-->

$$
\log_2(K) = \frac{2}{3} \left( 
            \log_2(\sqrt{2}\ \text{erfinv}(2 P_{\text{crit}}-1)) + \log_2(2^{m+3}) 
        \right)
        - \frac{log_2(N)}{3}
\tag{2}
$$

where 

- **erfinv** is the inverse error function.

- **P_crit** is fixed to our desired value, e.g. **0.55**

The solution for **log2(K)** is an upper bound on the lengths that satisfy **P_crit**, with **N** and **m** held constant.


## 4. Decision rules

Using equation 2, we can write the following code:

```
import numpy as np
from scipy.special import erfinv

def calculate_log2K(p_crit, m, log2_N = 0):
    """
    Calculate log2(K) using the formula:
    log2(K) = (2/3) * (log2(sqrt(2) * erfinv(2*P_crit - 1)) + m + 3) - log2(N)/3
    
    Args:
        p_crit (float): Critical probability value
        m (int): Column value (24, 32, or 64)
    
    Returns:
        float: Calculated log2(K) value
    """
    return (2/3) * (np.log2(np.sqrt(2) * erfinv(2*p_crit - 1)) + m + 2) - log2_N / 3
```


We write some more code to print out tables for specific values of ##m##,##N##, and ##P_crit##:

```
def show_call_validty(K, p_crit, N=1):
    def is_valid(n):
        if calculate_log2K(p_crit,n,np.log2(N)) >= np.log2(K):
            return "✓"
        else: return "✗"


    md = f"""

Call Validities for log2(K)={np.log2(K):.4f}, P_crit={p_crit}, log2(N)={np.log2(N):.4f} 
|call| valid | max allowed log2(K) |
| -- | -- | -- |
| `torch.argsort(torch.rand((batch_count,K)))` | {is_valid(24)} | {calculate_log2K(p_crit,24,np.log2(N)):.4f} |
| `torch.argsort(torch.randint(low=-2**31,high=2**31-1,(batch_count,K),dtype=torch.integer32))` | {is_valid(32)} | {calculate_log2K(p_crit,32,np.log2(N)):.4f} |
| `torch.argsort(torch.randint(low=-2**63,high=2**63-1,(batch_count,K),dtype=torch.integer64))` | {is_valid(64)} | {calculate_log2K(p_crit,64,np.log2(N)):.4f} |
"""
    
    display(Markdown(md))

show_call_validty(K=32*1024, p_crit=0.55, N=1)
show_call_validty(K=32*1024, p_crit=0.55, N=2**16)

display(Markdown("""

#### Explore the limits of using `randint` with 64 bits:

"""))

show_call_validty(K=32*1024*1024, p_crit=0.55, N=64*1024*1024)
show_call_validty(K=32*1024*1024, p_crit=0.50001, N=64*1024*1024)
show_call_validty(K=32*1024*1024, p_crit=0.500001, N=64*1024*1024)
```



Call Validities for log2(K)=15.0000, P_crit=0.55, log2(N)=0.0000 
|call| valid | max allowed log2(K) |
| -- | -- | -- |
| `torch.argsort(torch.rand((batch_count,K)))` | ✓ | 15.3384 |
| `torch.argsort(torch.randint(low=-2**31,high=2**31-1,(batch_count,K),dtype=torch.integer32))` | ✓ | 20.6717 |
| `torch.argsort(torch.randint(low=-2**63,high=2**63-1,(batch_count,K),dtype=torch.integer64))` | ✓ | 42.0051 |

Call Validities for log2(K)=15.0000, P_crit=0.55, log2(N)=16.0000 
|call| valid | max allowed log2(K) |
| -- | -- | -- |
| `torch.argsort(torch.rand((batch_count,K)))` | ✗ | 10.0051 |
| `torch.argsort(torch.randint(low=-2**31,high=2**31-1,(batch_count,K),dtype=torch.integer32))` | ✓ | 15.3384 |
| `torch.argsort(torch.randint(low=-2**63,high=2**63-1,(batch_count,K),dtype=torch.integer64))` | ✓ | 36.6717 |

#### Explore the limits of using `randint` with 64 bits:

Call Validities for log2(K)=25.0000, P_crit=0.55, log2(N)=26.0000 
|call| valid | max allowed log2(K) |
| -- | -- | -- |
| `torch.argsort(torch.rand((batch_count,K)))` | ✗ | 6.6717 |
| `torch.argsort(torch.randint(low=-2**31,high=2**31-1,(batch_count,K),dtype=torch.integer32))` | ✗ | 12.0051 |
| `torch.argsort(torch.randint(low=-2**63,high=2**63-1,(batch_count,K),dtype=torch.integer64))` | ✓ | 33.3384 |

Call Validities for log2(K)=25.0000, P_crit=0.50001, log2(N)=26.0000 
|call| valid | max allowed log2(K) |
| -- | -- | -- |
| `torch.argsort(torch.rand((batch_count,K)))` | ✗ | -1.5226 |
| `torch.argsort(torch.randint(low=-2**31,high=2**31-1,(batch_count,K),dtype=torch.integer32))` | ✗ | 3.8107 |
| `torch.argsort(torch.randint(low=-2**63,high=2**63-1,(batch_count,K),dtype=torch.integer64))` | ✓ | 25.1441 |

Call Validities for log2(K)=25.0000, P_crit=0.500001, log2(N)=26.0000 
|call| valid | max allowed log2(K) |
| -- | -- | -- |
| `torch.argsort(torch.rand((batch_count,K)))` | ✗ | -3.7372 |
| `torch.argsort(torch.randint(low=-2**31,high=2**31-1,(batch_count,K),dtype=torch.integer32))` | ✗ | 1.5961 |
| `torch.argsort(torch.randint(low=-2**63,high=2**63-1,(batch_count,K),dtype=torch.integer64))` | ✗ | 22.9295 |

As can be seen in the results under **"Explore the limits of using randint with 64 bits"**, using 64-bit version will produce unbiased result for all but the most extreme cases.

## 5. Conclusion

A decision rule for choosing how to generate batch permutations in PyTorch has been provided.  It is based on statistics, with two parameters **P_crit** and **N** to adjust for a usage case suitable risk model.

A more detailed explanation may be found https://github.com/craigphicks/pytorch-permutations-randperm-vs-argsort-rand/blob/main/README.md.
