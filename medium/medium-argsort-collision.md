# Permutation Generation in PyTorch on GPU: Statistic Based Decision Rule for `randperm` vs. `argsort` and `rand`

## 1. Introduction

The PyTorch library, widely used for neural networks on a GPU, supplies a function `torch.randperm(length)` that returns a permutation in a one-dimensional tensor, e.g.,
```python
>>> torch.randperm(6)
tensor([5, 3, 2, 4, 0, 1])
```
However, `torch.randperm` has a drawback - it cannot produce multiple permutations in batch mode. In PyTorch, batch mode refers to processing multiple inputs simultaneously, which improves performance.

A similar result can be obtained by using `torch.argsort` with `torch.rand` or `torch.randint`:
```python
>>> torch.argsort(torch.rand(6))
tensor([4, 3, 2, 5, 0, 1])
```
Unlike `torch.randperm`, `torch.argsort` with `torch.rand` or `torch.randint` can be used to produce a multidimensional tensor of permutations:
```python
# Create a batch of 3 permutations of length 6
>>> torch.argsort(torch.rand(3, 6))
tensor([[4, 3, 2, 5, 0, 1],
        [2, 5, 1, 0, 3, 4],
        [0, 3, 1, 5, 2, 4]])
```

The difference in speed becomes significant when utilizing a GPU to process tensors in parallel in a batch call, which may be faster per permutation by orders of magnitude. However, if there is no need to create multiple permutations or no need for speed, then using `randperm` is sufficient.

## 2. Bias in Permutations Generated by `torch.argsort` with `torch.rand`

How does `torch.randperm` differ from `torch.argsort(torch.rand())` and `torch.argsort(torch.randint())` and why is it considered better?

The issue is that among the arrays of random numbers produced by `torch.rand` and `torch.randint` are arrays that include duplicate numbers (also known as collisions). That, in turn, biases the distribution of permutations produced by sorting those arrays with `torch.sort`.

```python
>>> a = torch.randint(low=10, high=16, size=(6,))
tensor([11, 12, 15, 12, 11, 10])
>>> torch.sort(a, stable=True)
torch.return_types.sort(
values=tensor([10, 11, 11, 12, 12, 15]),
indices=tensor([5, 0, 4, 1, 3, 2]))
```

In the above example, the value of the parameter range `high - low` was set artificially low to 6 to induce duplicate values to be sorted. As a result, the sorted indices `0, 4`, corresponding to the duplicate sorted values `11, 11`, are arranged in ascending order (guaranteed by a stable sort, which maintains the relative order of equal elements). Likewise, indices `1, 3` are in ascending order. The resulting distribution of sorted indices is not symmetric because it is biased to have more ascending pairs than descending pairs, as a result of duplicates.

In contrast, the function `torch.randperm` is written to detect any duplicate keys and partially recalculate the results to remove the effects of those duplicate keys. The recalculation consists of regenerating keys for, and resorting only, those intervals with compromised values after the initial sort is complete. The result is almost perfect (excepting the tiny chance of the regenerated keys also having duplicates - it doesn't use a loop to eliminate that possibility). By using `torch.randperm`, the possibility of a biased distribution is (almost) eliminated.

To summarize:

- `randperm` ensures unbiased permutations but can't be batched efficiently on GPU
- `argsort(rand())` can be batched but has bias due to collisions

## 3. A criterion for evaluating the bias of `argsort` with `rand`/`randint`

Let **K** be the length of a permutation we want to generate.

Define a statistical measure **P_crit(K)** such that given a random permutation generated by either `randperm(K)` or `torch.argsort(torch.rand(K))` with a 50% chance of each method being used, but without knowing which method was used. **P_crit(K)** is the probability of guessing correctly which distribution the sample was drawn from, using an optimal guessing strategy.

We then constrain the value of **P_crit** to define our desired risk model. For example, setting **P_crit = 0.55** means we would be able to correctly guess whether a permutation came from the fair or biased distribution 11 out of 20 times, or 55% percent of the time. The possible range of **P_crit** is between **0.5** to **1**, corresponding to 50% to 100%. However, we will only consider the range **0.50** to **0.55**, because beyond that the model loses accuracy.

It can be shown that **P_crit** is adequately approximated by 

$$
P_{\text{crit}}(N,K,m) = \text{cdf}(\frac{\sqrt{NK}\ (K-1)}{2^{m+3}}; 0, 1)
\tag{1}
$$

where 

- **m** is the number of bits of the type used for random number generation, i.e., **24**, **32**, or **64**, corresponding to `rand(K)`, `randint(K,...,dtype=torch.integer32)`, or `randint(K,...,dtype=torch.integer64)`.

- **N** is a free parameter representing the number of permutations in the statistical ensemble to be evaluated. **N=1** is a typical value, even when more than one permutation is being generated.  (The number generated does not need to be equal to number being evaluated).

- **cdf(...;0,1)** is the cumulative distribution function of a unit guassian distribution.

Approximating $K-1$ as $K$, **log2(K)** is given by

<!-- 
eq2 = c - sqrt(2) * erfinv(2*p - 1)
eq8 = lnk + lnn/3 - 2/3 * (log(c) + log(s)) 
-->

$$
\log_2(K) = \frac{2}{3} \left( 
            \log_2(\sqrt{2}\ \text{erfinv}(2 P_{\text{crit}}-1)) + \log_2(2^{m+3}) 
        \right)
        - \frac{log_2(N)}{3}
\tag{2}
$$

where 

- **erfinv** is the inverse error function.

- **P_crit** is fixed to our desired value, e.g. **0.55**

The solution for **log2(K)** is an upper bound on the lengths that satisfy **P_crit**, with **N** and **m** held constant.


## 4. Decision rules





By fixing the value of **P_crit** at **P_crit = 0.55**, and **N=1**, can numerically solve for a decision rule to decide which values of **K** and **m** in **24,32,64** and suitable for use in batch computing of permutations.

The computed solution is as follows:


For the case of **P_crit = 0.55** and **N=1**, the decision rule is:

- if **log2(K) < 16.005** then **m=24**
- else if **log2(K) < 21.338** then **m=32**
- else if **log2(K) < 42.672** then **m=64**

The calls corresponding to each value of **m** in **24,32,64** are as follows:

- **m=24**: `argsort(rand(batch_count, K))`. 24 refers to the effective bits in torch.rand's floating point values.
- **m=32**: `argsort(randint(low=-2**31, high=2**31-1), (batch_count, K), dtype=torch.integer32)`
- **m=64**: `argsort(randint(low=-2**63, high=2**63-1), (batch_count, K), dtype=torch.integer64)`

> *Note: The PyTorch documentation for `randint` parameters says "high (int) – One above the highest integer to be drawn from the distribution." Therefore, we would expect the setting `high=2**31` to be appropriate. However, this has been found to lead to errors in some PyTorch versions, which is avoided by subtracting 1*

For example a permutation length value **K = 65,536 = 2¹⁶** could be satisfactorily computed using **m=24**.

A value **K = 2⁴²** is an impossibly long length for a permutation.  Therefore, every possible length could be satisfactorily computed using **m=32**.

In cases where the free parameter **N** is greater than 1, the decision rule is:

- if **log2(K) < 16.005 - log2(N)/3** then **m=24**
- else if **log2(K) < 21.338 - log2(N)/3** then **m=32**
- else if **log2(K) < 42.672 - log2(N)/3** then **m=64**

Notice the inclusion of the subtrahend **log2(N)/3**.  Clearly the earlier rule for **N=1** is just a special case of this more general rule.

Why not always set **N** to the number of permutation which will be generated?  Why can **N=1** be suitable even if huge numbers of permutations are being generated?  The answer is that evaluation may be based on a cost/benefit comparison per single permutation usage.  

This parable may be helpful to understand **N**: Suppose that improvements in health care meant you could live a million times longer, unless you died in a road accident.  Would you still ride a bike or drive on the road?  Some would answer "yes, just the same as before".  Those people would be setting **N=1**.  Others might say "never, far too risky". Those people would be setting **N=1,000,000**.  Yet others might choose some intermediate value for **N**.  It depends on the risk model - one size does not fit all.

## 5. Conclusion

A decision rule for choosing how to generate batch permutations in PyTorch has been provided.  It is based on statistics, with two parameters **P_crit** and **N** to adjust for a usage case suitable risk model.
The full mathematical exposition can be found at https://github.com/craigphicks/pytorch-permutations-randperm-vs-argsort-rand/blob/main/README.md.
