# Permutation Generation in PyTorch on GPU: Statistic Based Decision Rule for randperm vs. argsort and rand

## 1. Introduction

The PyTorch library, widely used for neural networks on a GPU, supplies a function `torch.randperm(length)` that returns a permutation in a one-dimensional tensor, e.g.,
```python
>>> torch.randperm(6)
tensor([5, 3, 2, 4, 0, 1])
```
However, `torch.randperm` has a drawback - it cannot produce multiple permutations in batch mode. In PyTorch, batch mode refers to processing multiple inputs simultaneously, which improves performance.

A similar result can be obtained by using `torch.argsort` with `torch.rand` or `torch.randint`:
```python
>>> torch.argsort(torch.rand(6))
tensor([4, 3, 2, 5, 0, 1])
```
Unlike `torch.randperm`, `torch.argsort` with `torch.rand` or `torch.randint` can be used to produce a multidimensional tensor of permutations:
```python
# Create a batch of 3 permutations of length 6
>>> torch.argsort(torch.rand(3, 6))
tensor([[4, 3, 2, 5, 0, 1],
        [2, 5, 1, 0, 3, 4],
        [0, 3, 1, 5, 2, 4]])
```

The difference in speed becomes significant when utilizing a GPU to process tensors in parallel in a batch call, which may be faster per permutation by orders of magnitude. However, if there is no need to create multiple permutations or no need for speed, then using `randperm` is sufficient.

## 2. Bias in Permutations Generated by `torch.argsort` with `torch.rand`

How does `torch.randperm` differ from and perform better than `torch.argsort(torch.rand...)`?

The issue is that among the arrays of random numbers produced by `torch.rand` and `torch.randint` are arrays that include duplicate numbers (also known as collisions). That, in turn, biases the distribution of permutations produced by sorting those arrays with `torch.sort`.

```python
>>> a = torch.randint(low=10, high=16, size=(6,))
tensor([11, 12, 15, 12, 11, 10])
>>> torch.sort(a, stable=True)
torch.return_types.sort(
values=tensor([10, 11, 11, 12, 12, 15]),
indices=tensor([5, 0, 4, 1, 3, 2]))
```

In the above example, the value of the parameter range `high - low` was set artificially low to 6 to induce duplicate values to be sorted. As a result, the sorted indices `0, 4`, corresponding to the duplicate sorted values `11, 11`, are arranged in ascending order (guaranteed by a stable sort, which maintains the relative order of equal elements). Likewise, indices `1, 3` are in ascending order. The resulting distribution of sorted indices is not symmetric because it is biased to have more ascending pairs than descending pairs, as a result of duplicates.

In contrast, the function `torch.randperm` is written to detect any duplicate keys and partially recalculate the results to remove the effects of those duplicate keys. The recalculation consists of regenerating keys for, and resorting only, those intervals with compromised values after the initial sort is complete. The result is almost perfect (excepting the tiny chance of the regenerated keys also having duplicates - it doesn't use a loop to eliminate that possibility). By using `torch.randperm`, the possibility of a biased distribution is (almost) eliminated.

## 3. A criterion for evaluating the bias of argsort with rand

An imbalance in ascending and pairs is only relatively bad.

Obviously, most length **K** random permutations generated by the unbiased function `randperm(K)` will have either more ascending or more descending adjacent pairs, although the mean is an equal count of ascending and descending adjacent pairs.

Likewise, most length **K** random permutations generated by the biased method  `torch.argsort(torch.rand(K))` will have either more ascending or more descending adjacent pairs, although the mean will be biased toward more ascending pairs.

We can model the distribtions of ascending and descending pairs as a binomial distribution of a coin toss.  Using this model the result of `randperm(K)` is represented as fair coin flipped **K** times, and the result of `torch.argsort(torch.rand(K))` is represented as a biased coin flipped **K** times, with the bias more likely to produce heads.

Then we can define a criterion **P_crit** based on the binomial coin toss model.

- Given a random permutaion generated by either `randperm(K)` or `torch.argsort(torch.rand(K))` with a 50% chance of each, let **P_crit(K)** be the probability of guessing correctly which one, using an optimal best guess strategy.

We then constraint the value of **P_crit** to define our desired risk model. For example, setting **P_crit = 0.55** means we would only be able correctly guess whether a permutaion came from the fair or biased distribtion 11 out of 20 times, or 55% percent of the time. The possible range of **P_crit** is between **0.5** to **1**, corresponding to 50% to 100%. 

The full mathemetical formulation is too much for this brief summary, but can be found at https://github.com/craigphicks/pytorch-permutations-randperm-vs-argsort-rand/blob/main/README.md.

Instead only the final decision rule is presented here, in the next section.

## 4. The Decision Rule

- **K** is the length of the permutation under consideration.
- **N** is a free parameter representing the number of permutations in the statistical ensemble to be evaluated. **N=1** is a typical value, even when more than one permutation is being generated.  

We will set the target **P_crit = 0.55**. 

For the case of **N=1**, the decision rule is:

- if **log2(K) < 16.005**
  - use the form `argsort(rand(batch_count, K))`
- else if **log2(K) < 21.338**
  - use the form `argsort(randint(low=-(2**31-1), high=2**31-1), (batch_count, K), dtype=torch.integer32)`
- else if **log2(K) < 42.672**
  - use the form `argsort(randint(low=-(2**63-1), high=2**63-1), (batch_count, K), dtype=torch.integer64)`
- else 
  - use the form `randperm(K)`

In case the free parameter **N** is greater than 1, the decision rule is:

- if **log2(K) < 16.005 - log2(N)/3**
  - use the form `argsort(rand(batch_count, K))`
- else if **log2(K) < 21.338 - log2(N)/3**
  - use the form `argsort(randint(low=-(2**31-1), high=2**31-1), (batch_count, K), dtype=torch.integer32)`
- else if **log2(K) < 42.672 - log2(N)/3**
  - use the form `argsort(randint(low=-(2**63-1), high=2**63-1), (batch_count, K), dtype=torch.integer64)`
- else 
  - use the form `randperm(K)`

> *\* Note 1: The PyTorch documentation for `randint` parameters says "high (int) â€“ One above the highest integer to be drawn from the distribution." Therefore, we would expect the setting `high=2**31` to be appropriate. However, this has been found to lead to errors in some PyTorch versions, which is avoided by subtracting 1*

Notice that the threshhold is being applied to **log2(K)**, not to **K**.  Therefore, for the case of **N=1**, it would be acceptable to generate permutations of length 65,536 (=2**16) using `argsort(rand(batch_count, K))`.

Why not always set **N** to the number of permutation which will be generated?  Why can **N=1** be suitable even if huge numbers of permutations are being generated?  The answer is that evaluation may be based on a cost/benefit comparison per single permutation usage.  

This parable may be helpful to understand **N**: Suppose that improvements in health care meant you could live a million times longer, unless you died in a road accident.  Would you still ride a bike or drive on the road?  Some would answer "yes, just the same as before".  Those people would be setting **N=1**.  Others might say "never, far too risky". Those people would be setting **N=1,000,000**.  Yet others might choose some intermediate value for **N**.  It depends on the risk model - one size does not fit all.

## 5. Conclusion

A decision rule for choosing how to generate batch permutations in pytorch has been provided.  It is based on statistics, with two parameters **P_crit** and **N** to adjust for a usage case suitable risk model.
The full mathematical exposition can be found at https://github.com/craigphicks/pytorch-permutations-randperm-vs-argsort-rand/blob/main/README.md.





